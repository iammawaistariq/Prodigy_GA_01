{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9ujyuMN9kCOm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRH4Ra7FkDAs"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ztch04N2j579",
        "outputId": "4206ef95-e101-4b8e-9217-d31417e867e0"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-U9ngXRRQH"
      },
      "source": [
        "# Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wqZc78vC2XfX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, AutoModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "buSOe-sH5u7g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX4xhb_RL3T2",
        "outputId": "5c0f4de9-596f-42e8-9244-4ad1787cf67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted files to /content/job_applications\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def extract_zip_file(zip_file_path, extract_to_folder):\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_folder)\n",
        "    print(f\"Extracted files to {extract_to_folder}\")\n",
        "\n",
        "# Example usage\n",
        "zip_file_path = '/content/job_applications.zip'  # Update this path as needed\n",
        "extract_to_folder = '/content/job_applications'\n",
        "extract_zip_file(zip_file_path, extract_to_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TEMyAC8YL3Ra"
      },
      "outputs": [],
      "source": [
        "def load_and_merge_text_files(folder_path, delimiter=\"\\n---\\n\"):\n",
        "\n",
        "    all_texts = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read().strip()\n",
        "                all_texts.append(text)\n",
        "    merged_text = delimiter.join(all_texts)\n",
        "    return merged_text\n",
        "\n",
        "# Example usage\n",
        "extracted_folder = '/content/job_applications'\n",
        "merged_dataset = load_and_merge_text_files(extracted_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R-9C6VFCL3Ow"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmfR_km85u5O",
        "outputId": "90fae8db-7f28-440b-d559-47df1750a4ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged dataset to /content/merged_job_applications.txt\n"
          ]
        }
      ],
      "source": [
        "def save_merged_dataset(merged_text, output_file_path):\n",
        "\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(merged_text)\n",
        "    print(f\"Saved merged dataset to {output_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "output_file_path = '/content/merged_job_applications.txt'\n",
        "save_merged_dataset(merged_dataset, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "12x2I0UA5u3G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-6cWMGB5u0y",
        "outputId": "8cff9764-e664-4c74-ab0d-d2dff776903b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, GPT2Tokenizer\n",
        "\n",
        "def load_training_dataset(tokenizer, file_path, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Example usage\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "training_dataset = load_training_dataset(tokenizer, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rsSPSUqx5uyP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2Tokenizer\n",
        "\n",
        "def fine_tune_gpt2_model(model_name, train_dataset, output_dir=\"./gpt2_finetuned\"):\n",
        "    # Disable wandb logging\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        "        report_to=\"none\"  # Disable reporting to wandb or any other tracking tool\n",
        "    )\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Model fine-tuned and saved at {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "xRYaw8RD5uvf",
        "outputId": "9bcd3e95-234d-4f94-95e0-21d146628f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [39/39 04:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model fine-tuned and saved at ./gpt2_finetuned\n"
          ]
        }
      ],
      "source": [
        "model_name = \"gpt2\"\n",
        "fine_tune_gpt2_model(model_name, training_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Nu1Jctsh5_cA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "skWiSOyCPxjI"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Define the path to the fine-tuned model directory\n",
        "model_path = \"./gpt2_finetuned\"\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def generate_email(prompt, max_length=200):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    # Generate text with adjusted parameters\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,  # Generate only one email\n",
        "            no_repeat_ngram_size=3,  # Avoid repeated phrases\n",
        "            do_sample=True,\n",
        "            top_k=40,    # Limit token selection to top-k options\n",
        "            top_p=0.75,  # Narrow down token probability selection\n",
        "            temperature=0.6,  # Lower temperature for less randomness\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Post-processing to clean up the generated text\n",
        "    lines = [line.strip() for line in generated_text.split(\"\\n\") if line.strip()]\n",
        "    cleaned_text = \"\\n\\n\".join(lines[:6])  # Limit the output to a reasonable length\n",
        "\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqZzTZ6RoNQJ"
      },
      "source": [
        "# Generating New text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ReDJzN6jHE",
        "outputId": "c4066a91-b230-4e4a-9a95-fced0fde67e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Email:\n",
            "\n",
            "\n",
            "Subject: Application for the Machine Learning Position\n",
            "\n",
            "Dear Hiring Manager,\n",
            "\n",
            "I am applying for the Computer Science position at Microsoft Research. I am a passionate computer science student and have developed a strong analytical skills that have led me to a successful career in data analysis and machine learning. I believe that my analytical skills and analytical skills are valuable to my team, and I look forward to your consideration.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Michael H. Williams\n",
            "\n",
            "(555) 567-2389\n"
          ]
        }
      ],
      "source": [
        "prompt_text = \"Subject: Application for the Machine Learning Position\"\n",
        "generated_email = generate_email(prompt_text, max_length=250)\n",
        "\n",
        "print(f\"Generated Email:\\n\\n\\n{generated_email}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "N9Y31SlX6jBj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0GU0nq-_6i_E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
